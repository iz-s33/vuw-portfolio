---
title: "DATA 303 Assignment 2"
author: "Izzy Southon, 300597453"
date:   "Due 11:59pm Friday 19 April"
output: pdf_document
---

## Assignment Questions

**Q1.(26 marks)**   The dataset `fiat.csv`contains 1538 records on sales of used Fiat 500 cars in Italy.  The variables in the dataset are:

*  `model`: Fiat 500 comes in several 'flavours' :'pop', 'lounge', 'sport'
*  `power`: number of KW of the engine
*  `age`: age of the car in number of days (at the time dataset was created)
*  `km`: Distance travelled by the car in kilometers
*  `owners`: number of previous owners
*  `lat`: latitude of the seller (the price of cars in Italy varies from North to South of the country)
*  `lon`: longitude of the seller 
*  `price`: selling price (in Euro)

In this question, we are interested in identifying the key predictors of `price`, and in understanding how these predictors affect `price`. Model interpretability is important in this case. The initial steps in building a model for `price` are shown in the Appendix on pages 3 to 6.

As there is evidence of non-normality and non-constant variance, a log-transformation for `price` is to be applied in the rest of the analyses. Prepare the data as has been done in the Appendix, and use your new dataset to answer the questions below.

```{r message = FALSE}
library(dplyr)
library(memisc)
```

```{r}
fiat<-read.csv("fiat.csv", header=T)
str(fiat)
```

```{r}
## Changing owners and power into categorical variables as they have very few unique values
fiat$owners<-as.factor(fiat$owners)
fiat<-fiat%>%
  mutate(power.cat=memisc::recode(power,"50-59"<-c(50:59),
                                "60-69"<-c(60:69),
                                "70-79"<-c(70:79)))%>%
  dplyr::select(-power)
str(fiat)
```

a.  **(4 marks)**  Fit a generalised additive model for `log(price)`, including all predictors used in `fit1` in the Appendix.  Use a smooth spline for each numerical predictor.  Comment on the non-linearity and significance of all smooth terms.

$Y = \beta_0 + s_1(X_1) + s_2(X_2) +...+ s_p(X_p)+ \epsilon$

The fitted model equation where Y = log(price) is:

$log(price)= 9.0346 + 3.78(age) + 2.92(km) + 4.54(lat) + 4.34(lon) + \epsilon$

```{r message = FALSE}
library(mgcv)
library(pander)
```

```{r}
# Multiple regression with GAMs - GAMs with multiple predictors

# Fitting the GAM model for `log(price)`
fit.gam<-gam(log(price)~model + owners + power.cat + s(age) + s(km) + s(lat) 
             + s(lon), data=fiat, method="REML")

summ.gam<-summary(fit.gam)
summ.gam
# Putting GAM smooth terms into a nice looking table 
pander(summ.gam$s.table,digits=3)
```

In the model labelled 'fit', there are smooth terms for 4 predictors (`age`, `km`, `lat`, & `lon`). Based on the plots below and the values in the table above, 'age', 'lat' & 'lon' are all non-linear and significant. This is because all smooth terms have a high edf and a low p-value. 'km' has a low edf and a low p-value, meaning that km is linear and significant. Since the p-values for all terms are less than 0.05, we reject the null hypothesis and conclude that the smooth terms for age, km, lat & lon are significant. 

Hypotheses

$H_0:$ the smooth term is not significant 

$H_1:$ the smooth term is significant 


```{r}
par(mfrow=c(2,2)) 
plot(fit.gam)
```


b.  **(5 marks)** Perform a diagnostic check of regression assumptions and adequacy of basis functions for the model you fitted in part (a).  What conclusions do you draw from your results?

```{r}
# Model checking - residual diagnostics & basis functions 
par(mfrow=c(2,2))
gam.check(fit.gam, k.rep=1000)
```

**Regression assumptions:** 


**Q-Q plot:** This plot shows that there seems to be some non-normality present. Deviance residuals heavily deviate from the Q-Q line at the beginning and the end. 

**Resids vs. linear pred plot:** The residuals are relatively equally spread without a distinct pattern. This is good indication that there are no uncaptured signficant non-linear relationships. 

**Histogram of residuals plot:** This plot is showing that residuals are normally distributed.

**Response vs. Fitted values plot:** This plot shows the overall fit of the model. It looks like constant variance and linearity are present. 

**Adequacy of basis functions:**

The output above reports full convergence, indicating that an optimal solution has been found. The maximum number of basis functions considered (k') is 9. Small p-values indicate that residuals are not randomly distributed. In the model, the p-values for all smooth terms (`age`, `km`, `lat`, & `lon`) are relatively low (lowest for lat and lon), and k-index is close to 1 for all smooth terms. The edf for all smooth terms is a lot lower than k', indicating that we are likely to have adequate numbers of basis functions. Refitting the model with higher k values for `age`, `km`, `lat`, & `lon` could be considered to be sure that there is enough basis functions for all smooth terms.

c.   **(4 marks)** For ease of interpretation, a linear model is preferred to a GAM.  Fit a linear model (using `lm`) for `log(price)` with predictors as shown in model `fit1`.  Based on your fitted model, give a mathematical interpretation of the effect of `age` on `price`. 

```{r}
fit1<-lm(log(price) ~ model + power.cat + age + km + owners + lat + lon, data=fiat)
pander(summary(fit1), caption="")
```

Fitted model: 
$\widehat{log(price)} = 9.108 - 0.037modelpop - 0.022modelsport + 0.021power.cat_{60-69} + \\ 0.002power.cat_{70-79} - 0.0001145age - 2.449 * 10^-6km + 0.00314owners_{2} + \\ 0.01189owners_{3} - 0.01367owners{4} + 0.0052lat + 0.001693lon$

**Mathematical interpretation of the effect of `age` on `price`:**

The regression coefficient for `age` is $e^-0.0001145-1=-0.00011445$. Therefore, we expect the selling price in euros (â‚¬) to decrease by a multiplicative factor of 0.00011445 for each additional day the car ages. When age decreases by 1 while holding all other predictors constant we expect: change in price $=$ price $\times -0.0001145$. 

d.   **(5 marks)**  Use the `step` function to perform stepwise model selection for the model in part (c) based on $AIC$ and $BIC$, to determine whether any of the predictors can be excluded from the model. List the predictors included in your preferred model in each case and justify your answer.

```{r}
##AIC
step(fit1, direction = "both")
```

The 'starting' model is the one that includes all predictors and has $AIC = -6997.5$. From the output above we can see that when the predictors lon, power.cat, and owners are included in the model, we get an increase in AIC. When the predictors lat, model, km, and age are removed from the model, we also get an increase in AIC. 

$AIC(A)-AIC(B)=-6996.4-(-6997.5)=1.1$

The difference is in the [0,2.5) interval. Therefore, applying the AIC rules of thumb means there is no difference in models, and we prefer the model with fewer predictors using the principle of parsimony. 

The preferred model according to the AIC criterion includes the four predictors: `model, age, km` and `lat`. 

```{r}
##BIC
step(fit1, direction="both", k=log(nrow(fiat)))
```

The model that includes all predictors is to be labelled model B, and the model with the next largest BIC value (excludes `lat`) is labelled model A.

$BIC(A)-BIC(B)=-6963.4-(-6965.5)=2.1$

As the difference is within the interval of [2.0,6.0), using Raftery BIC rules of thumb means there is positive preference for model B, the model with a smaller BIC value. Therefore, we opt for the model with more predictors, model B. Therefore, the preferred model according to the BIC criterion includes all predictors: `model`, `power.cat`, `age`, `km`, `owners`, `lat`, and `lon`.

e.  **(4 marks)**  It is known that the price of cars in Italy varies from North to South of the country.  You also suspect that the effect of `lat` varies by `model`, and you therefore investigate the interaction between `lat` and `model`.  Add the interaction `model:lat` to the preferred model based on $AIC$ in part (d). Obtain an interaction plot and use it to describe briefly the effect of `lat` on `log(price)`

```{r message = FALSE}
library(interactions)
```

```{r}
## Adding an interaction term (model:lat) to the preferred model based on AIC
fit2<-lm(log(price) ~ model + age + km + lat + model:lat, data=fiat)
pander(summary(fit2)$coefficients, caption="")

## Interaction plot
interact_plot(fit2,pred=lat, modx=model,
              colors = "Qual1", data=fiat)

addmargins(table(fiat$model))
```

The interaction plot above shows that the effect of `lat` varies by model. 

Null hypothesis $H_0: lat = 0$ (there is no interaction)

Alternative hypothesis $H_1: lat \neq 0$ (there is an interaction)

For there interaction between modelpop and lat, the p-value of 0.0027 < 0.05, thus there is evidence to reject the null hypothesis and conclude there is an interaction between modelpop and lat. For the interaction between modelsport and lat, the p-value of 0.2585 > 0.05, thus there is not enough evidence to reject the null hypothesis, therefore there is not enough evidence to conclude an interaction between modelsport and lat. 

The plot shows that for model(lounge), an increase in lat is associated with an increase in (log)price. For model(pop), an increase in lat is associated with a reduction in log(price). For model(sport), an increase in lat is associated with no change in log(price). This might be due to there being 86 observations in the dataset for modelsport. Modelsport has the least amount of observations, as modellounge has 1094 and modelpop has 358 observations. 

f.  **(4 marks)** Obtain and print in a table, the $AIC$ and $BIC$ values for your $AIC$-based preferred model in part (d) and the model in part (e).  Based on these values, state whether or not you would include the interaction term in your final preferred model and justify your answer.

```{r}
# The model fit2 has the interaction term
# The model fit3 does not have the interaction term

## Fitting the model from part d (the AIC-based preferred model)
fit3<-lm(log(price) ~ model + age + km + lat, data=fiat) 

##AIC
pander(AIC(fit2, fit3), caption="")

##BIC
pander(BIC(fit2, fit3), caption="")
```

Based off of the AIC and BIC values, results show that the model with the interaction term (fit 2) (lower AIC) is the preferred model according to AIC, while the model without the interaction term (fit3) (lower BIC) is the preferred model according to BIC.

Since we are aiming to find the key predictors of price and in understanding how these how these predictors affect price in the fiat dataset, this is an inference problem. We can use adjusted $R^2$ to find out which model is preferred. 

```{r}
fit2.adjrsq<-summary(fit2)$r.sq
fit3.adjrsq<-summary(fit3)$r.sq

modname<-c("fit2", "fit3")
adj.rsq<-c(fit2.adjrsq,fit3.adjrsq)
rsq<-data.frame(modname,adj.rsq)
pander(rsq, caption="")
```

We see that the model with the interaction term (fit2) has a slightly higher adjusted $R^2$ compared to the model without the interaction term (fit3). We opt for the model with the interaction term as the preferred model. 

**Q2. (14 marks)**  In this next question we'll focus on constructing a prediction model for the Fiat data using subset selection and shrinkage methods.

a. **[6 marks ]** Use the `olsrr` package to perform best subset, forward and backward stepwise model selection for the Fiat data. In each case, use AIC as the model performance metric to base your selection on and list the predictors in your final model.

```{r message = FALSE}
library(olsrr)
```

```{r}
# Using the fiat data, I fit a linear model that includes all predictors 
fit.full<-lm(price ~ ., data = fiat)
```

```{r}
# Best subset 
best.subset<-ols_step_best_subset(fit.full)
# We now have the best models M1,...M7 with 1, 2, 3,...,7 predictors
sub.aic<-best.subset$aic # Select the best model out of the models shown in best.subset using AIC metric
which.min(sub.aic) # This gives the model with the lowest AIC
pander(best.subset$predictors[4])
```

The model with the lowest AIC and therefore the best model selected using best subset includes 4 predictors (model, age, km, and lat) (M4). The AIC for this model (M4) is 24779.96, yet the model (M5) with the second lowest AIC includes model, power.cat, age, km, and lat. M5 obtained an AIC of 24780.51, which is not much higher than the model with the lowest AIC. This is due to the AIC's penalty for model complexity 2(p+1). While adding more predictors (in this case model.cat) can sometimes improve model fit, it can also lead to overfitting or increased complexity, so we prefer M4 with fewer predictors. 

```{r}
# Forward selection
forward.stepwise<-ols_step_forward_aic(fit.full)
forward.stepwise
plot(forward.stepwise)
ols_step_forward_aic(fit.full)$model 
```

The best model for forward stepwise model selection includes 4 predictors being age, km, model, and lat, where the predictors are added to the model in this order. The plot shows the variables added to the model at each step (starting from a null model with no predictors). 

```{r}
# Backward selection
backward.stepwise<-ols_step_backward_aic(fit.full)
backward.stepwise
plot(backward.stepwise)
ols_step_backward_aic(fit.full)$model
```

The best model for backward stepwise model selection includes 4 predictors being model, age, km, and lat (backward selection found this by excluding owners, lon, and power.cat when starting from a full model that included all predictors). The predictors were excluded from the model in the order: owners, lon, power.cat.


The final model includes 4 predictors being `model`, `age`, `km`, and `lat`. This is because when using AIC as the performance metric, best subset selection, forward selection, and backward selection all chose the same model. 

b. **[3 marks]**  Apply ridge regression to the Fiat data and use cross-validation to identify the "best" value, $\lambda_{MSEmin}$, for the penalty parameter $\lambda$.  In a table, print the coefficients for a model fitted  using $\lambda_{MSEmin}$ and a model fitted using $\lambda=0$.  Based on your table are there any predictors that you would consider for exclusion?  Explain your answer briefly. 

```{r message = FALSE}
library(glmnet)
```

```{r}
## Create the design matrix
x<-model.matrix(price~.,fiat)
x[1:3,]

## Exclude the column of 1's that represent the intercept
x<-x[, -1]
x[1:3,]

## Create the y vector
y<-fiat$price

## Fit the ridge regression model
ridge.mod<-glmnet(x,y,alpha=0) # alpha=0 for ridge regression

## Using CV to identify the "best" value
set.seed(1)
cv_ridge <- cv.glmnet(x = x,y = y, alpha = 0)
plot(cv_ridge)

cv_ridge$lambda.min ## best value of the penalty parameter lambda

## Fit model using "best" lambda
ridge.mod.best <-glmnet(x,y, alpha = 0,lambda = cv_ridge$lambda.min)

## Fit model using lambda=0
ridge.mod.zero <-glmnet(x,y, alpha = 0,lambda = 0)

## Produce table of coefficients of both models
pander(data.frame("Best" = coef(ridge.mod.best)[, 1],
"Lambda=0" = coef(ridge.mod.zero)[, 1]),
col.names = c("Best", "Lambda=0"))
```

All estimated coefficients are non-zero, indicating that all predictors are required for the model, regardless of which values for $\lambda$ was used. Coefficents cannot be completely shrunk to zero with ridge regression unless $\lambda = infinity$. A sensible choice is $\lambda_{MSEmin} = 173.2455$. Shrinkage has not had the same impact on all regression coefficients. For example for modelsport, as lambda decreased to zero, shrinkage toward zero happens, whereas for lat, as lambda was decreased to zero, the coefficient increased.  

c. **[3 marks]** Apply lasso regression to the Fiat data and use cross-validation to identify the "best" value, $\lambda_{MSEmin}$, for the penalty parameter $\lambda$.  In a table, print the coefficients for a model fitted  using $\lambda_{MSEmin}$ and a model fitted using $\lambda=0$.  Based on your table identify predictors that you would consider for exclusion.  Explain your answer briefly. 

```{r}
## Fit lasso regression model
lasso.mod <- glmnet(x, y, alpha = 1) 
set.seed(1)
cv_lasso <- cv.glmnet(x = x,y = y, alpha = 1)
plot(cv_lasso)

cv_lasso$lambda.min

## Fit model using "best" lambda
lasso.mod.best <-glmnet(x,y, alpha = 1,lambda = cv_lasso$lambda.min)

## Fit model using lambda=0
lasso.mod.zero <-glmnet(x,y, alpha = 1,lambda = 0)

## Produce a table
pander(data.frame("Best" = coef(lasso.mod.best)[, 1],
"Lambda=0" = coef(lasso.mod.zero)[, 1]),
col.names = c("Best", "Lambda=0"))
```

When the model is using $\lambda_{MSEmin}$, coefficients are shrunk to zero for the predictors modelsport, owners2, owners3, owners4, lon, and power.cat60-69. This means that when using the 'best' $\lambda$, these predictors can be excluded from the model. When $\lambda = 0$, no predictors can be excluded from the model, as no coefficients are completely shrunk to zero.  

d. **[2 marks]**  Based on your results in parts (b) and (c), which of the two approaches, ridge regression or lasso regression, would you prefer to use for model selection.  Explain your answer briefly.

I would prefer to use lasso regression over ridge regression for model selection because the aim is to select the best predictors. Ridge regression is not used to select predictors, but is a shrinkage method that shrinks coefficients. Ridge regression does not create a sparser model, and has no clear way of indicating which predictors to be excluded. Therefore, lasso regression is better to use for model selection as it is more likely to be able to find the best predictive model as coefficients can be completely shrunk to zero, thus sparse models can be created. Additionally, lasso regression somewhat aligns with the results obtained with backward selection, where lon can be excluded from the model, and levels of owners and power.cat can be excluded. With ridge regression, predictors have not been excluded like lasso regression and backward selection have. 


**Assignment total: 40 marks**

---------------





